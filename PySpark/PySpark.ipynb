{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up PySpark"
      ],
      "metadata": {
        "id": "PHMT9uYVkJpB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVrnuBrtkEYy",
        "outputId": "33dca3a9-8f3c-474b-f929-434445b17407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=3e9457998d6588df1dade26c77f847cf8ecc0e1194d30bc732ba54de23849770\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark using pip\n",
        "!pip install pyspark\n",
        "\n",
        "# Set environment variables for Spark\n",
        "!export SPARK_HOME=/path/to/spark\n",
        "!export PATH=$SPARK_HOME/bin:$PATH\n",
        "!export PYSPARK_PYTHON=python3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Operations and Transformations"
      ],
      "metadata": {
        "id": "6l7vYE_bkPq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Spark Session:\n",
        "\n"
      ],
      "metadata": {
        "id": "hY8qIrVcofzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"BasicOperations\").getOrCreate()"
      ],
      "metadata": {
        "id": "CWjBTVvKoYC7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data:"
      ],
      "metadata": {
        "id": "DWfuv_J0ojYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"dataset.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "apDIzJTQoj6A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame Operations:"
      ],
      "metadata": {
        "id": "hy70vQJfosXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 5 rows\n",
        "df.show(5)\n",
        "\n",
        "# Print schema of the DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Select specific columns\n",
        "df.select(\"transaction_id\", \"customer_id\").show()\n",
        "\n",
        "# Filter rows\n",
        "df.filter(df[\"quantity\"] > 1).show()\n",
        "\n",
        "# Group by and aggregate\n",
        "df.groupBy(\"product_id\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3oLfxxSouQY",
        "outputId": "81e17039-f58f-4567-e291-93a86c8d8412"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------------+----------+--------+-----+\n",
            "|transaction_id|customer_id|transaction_date|product_id|quantity|price|\n",
            "+--------------+-----------+----------------+----------+--------+-----+\n",
            "|             1|       1001|      2023-01-01|      2001|       2| 20.0|\n",
            "|             2|       1002|      2023-01-02|      2003|       1| 15.0|\n",
            "|             3|       1001|      2023-01-03|      2002|       3| 10.0|\n",
            "|             4|       1003|      2023-01-04|      2001|       1| 20.0|\n",
            "|             5|       1002|      2023-01-05|      2003|       2| 15.0|\n",
            "+--------------+-----------+----------------+----------+--------+-----+\n",
            "\n",
            "root\n",
            " |-- transaction_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_date: date (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            "\n",
            "+--------------+-----------+\n",
            "|transaction_id|customer_id|\n",
            "+--------------+-----------+\n",
            "|             1|       1001|\n",
            "|             2|       1002|\n",
            "|             3|       1001|\n",
            "|             4|       1003|\n",
            "|             5|       1002|\n",
            "+--------------+-----------+\n",
            "\n",
            "+--------------+-----------+----------------+----------+--------+-----+\n",
            "|transaction_id|customer_id|transaction_date|product_id|quantity|price|\n",
            "+--------------+-----------+----------------+----------+--------+-----+\n",
            "|             1|       1001|      2023-01-01|      2001|       2| 20.0|\n",
            "|             3|       1001|      2023-01-03|      2002|       3| 10.0|\n",
            "|             5|       1002|      2023-01-05|      2003|       2| 15.0|\n",
            "+--------------+-----------+----------------+----------+--------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|product_id|count|\n",
            "+----------+-----+\n",
            "|      2003|    2|\n",
            "|      2001|    2|\n",
            "|      2002|    1|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame Transformations:"
      ],
      "metadata": {
        "id": "1qp0kURMpHSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column for total amount\n",
        "df = df.withColumn(\"total_amount\", df[\"quantity\"] * df[\"price\"])\n",
        "\n",
        "# Drop a column\n",
        "df = df.drop(\"product_id\")\n",
        "\n",
        "# Rename a column\n",
        "df = df.withColumnRenamed(\"transaction_date\", \"date\")"
      ],
      "metadata": {
        "id": "ruGSW3rQpIKi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning with PySpark"
      ],
      "metadata": {
        "id": "03-LSsHPpRVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Feature engineering\n",
        "assembler = VectorAssembler(inputCols=[\"quantity\", \"price\"], outputCol=\"features\")\n",
        "data = assembler.transform(df)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Initialize a linear regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
        "\n",
        "# Fit the model on training data\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Predict on test data\n",
        "predictions = lr_model.transform(test_data)\n",
        "predictions.select(\"total_amount\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLwDwUAPpVfI",
        "outputId": "db9ba8d7-6c81-45e2-9ef4-77941bf0dfec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------------+\n",
            "|total_amount|        prediction|\n",
            "+------------+------------------+\n",
            "|        15.0| 5.000000000000348|\n",
            "|        30.0|25.000000000000107|\n",
            "+------------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}